{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "821c0e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of files in the directory:\n",
      "Base.csv\n",
      "Variant I.csv\n",
      "Variant II.csv\n",
      "Variant III.csv\n",
      "Variant IV.csv\n",
      "Variant V.csv\n"
     ]
    }
   ],
   "source": [
    "# create a list of all the files in the directory artifacts\\data\\kagglebankfraud\n",
    "import os\n",
    "\n",
    "# Set the root directory path\n",
    "root_directory = r'C:\\Users\\i_bau\\OneDrive\\aaaWork\\repos\\frauprev'  # Replace with your actual root directory path\n",
    "\n",
    "# Get the list of files in the directory within the root directory\n",
    "# Set the directory path relative to the root directory\n",
    "directory = os.path.join(root_directory, r'data\\kagglebankfraud')\n",
    "# Get the list of files in the directory\n",
    "allfiles = os.listdir(directory)\n",
    "# Print the list of files\n",
    "\n",
    "files = [f for f in allfiles if os.path.isfile(os.path.join(directory, f))]  # and ('III' in f or ' V' in f)]\n",
    "print(\"List of files in the directory:\")\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e87de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added account_id column to Base.csv and saved the changes.\n",
      "Added account_id column to Variant I.csv and saved the changes.\n",
      "Added account_id column to Variant II.csv and saved the changes.\n",
      "Added account_id column to Variant III.csv and saved the changes.\n",
      "Added account_id column to Variant IV.csv and saved the changes.\n",
      "Added account_id column to Variant V.csv and saved the changes.\n",
      "List of files with account_id added:\n",
      "Base.csv\n",
      "Variant I.csv\n",
      "Variant II.csv\n",
      "Variant III.csv\n",
      "Variant IV.csv\n",
      "Variant V.csv\n",
      "\n",
      "First few rows of each file:\n",
      "File: Base.csv\n",
      "Column names: ['account_id', 'fraud_bool', 'income', 'name_email_similarity', 'prev_address_months_count', 'current_address_months_count', 'customer_age', 'days_since_request', 'intended_balcon_amount', 'payment_type', 'zip_count_4w', 'velocity_6h', 'velocity_24h', 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'employment_status', 'credit_risk_score', 'email_is_free', 'housing_status', 'phone_home_valid', 'phone_mobile_valid', 'bank_months_count', 'has_other_cards', 'proposed_credit_limit', 'foreign_request', 'source', 'session_length_in_minutes', 'device_os', 'keep_alive_session', 'device_distinct_emails_8w', 'device_fraud_count', 'month']\n",
      "\n",
      "\n",
      "File: Variant I.csv\n",
      "Column names: ['account_id', 'fraud_bool', 'income', 'name_email_similarity', 'prev_address_months_count', 'current_address_months_count', 'customer_age', 'days_since_request', 'intended_balcon_amount', 'payment_type', 'zip_count_4w', 'velocity_6h', 'velocity_24h', 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'employment_status', 'credit_risk_score', 'email_is_free', 'housing_status', 'phone_home_valid', 'phone_mobile_valid', 'bank_months_count', 'has_other_cards', 'proposed_credit_limit', 'foreign_request', 'source', 'session_length_in_minutes', 'device_os', 'keep_alive_session', 'device_distinct_emails_8w', 'device_fraud_count', 'month']\n",
      "\n",
      "\n",
      "File: Variant II.csv\n",
      "Column names: ['account_id', 'fraud_bool', 'income', 'name_email_similarity', 'prev_address_months_count', 'current_address_months_count', 'customer_age', 'days_since_request', 'intended_balcon_amount', 'payment_type', 'zip_count_4w', 'velocity_6h', 'velocity_24h', 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'employment_status', 'credit_risk_score', 'email_is_free', 'housing_status', 'phone_home_valid', 'phone_mobile_valid', 'bank_months_count', 'has_other_cards', 'proposed_credit_limit', 'foreign_request', 'source', 'session_length_in_minutes', 'device_os', 'keep_alive_session', 'device_distinct_emails_8w', 'device_fraud_count', 'month']\n",
      "\n",
      "\n",
      "File: Variant III.csv\n",
      "Column names: ['account_id', 'fraud_bool', 'income', 'name_email_similarity', 'prev_address_months_count', 'current_address_months_count', 'customer_age', 'days_since_request', 'intended_balcon_amount', 'payment_type', 'zip_count_4w', 'velocity_6h', 'velocity_24h', 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'employment_status', 'credit_risk_score', 'email_is_free', 'housing_status', 'phone_home_valid', 'phone_mobile_valid', 'bank_months_count', 'has_other_cards', 'proposed_credit_limit', 'foreign_request', 'source', 'session_length_in_minutes', 'device_os', 'keep_alive_session', 'device_distinct_emails_8w', 'device_fraud_count', 'month', 'x1', 'x2']\n",
      "\n",
      "\n",
      "File: Variant IV.csv\n",
      "Column names: ['account_id', 'fraud_bool', 'income', 'name_email_similarity', 'prev_address_months_count', 'current_address_months_count', 'customer_age', 'days_since_request', 'intended_balcon_amount', 'payment_type', 'zip_count_4w', 'velocity_6h', 'velocity_24h', 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'employment_status', 'credit_risk_score', 'email_is_free', 'housing_status', 'phone_home_valid', 'phone_mobile_valid', 'bank_months_count', 'has_other_cards', 'proposed_credit_limit', 'foreign_request', 'source', 'session_length_in_minutes', 'device_os', 'keep_alive_session', 'device_distinct_emails_8w', 'device_fraud_count', 'month']\n",
      "\n",
      "\n",
      "File: Variant V.csv\n",
      "Column names: ['account_id', 'fraud_bool', 'income', 'name_email_similarity', 'prev_address_months_count', 'current_address_months_count', 'customer_age', 'days_since_request', 'intended_balcon_amount', 'payment_type', 'zip_count_4w', 'velocity_6h', 'velocity_24h', 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'employment_status', 'credit_risk_score', 'email_is_free', 'housing_status', 'phone_home_valid', 'phone_mobile_valid', 'bank_months_count', 'has_other_cards', 'proposed_credit_limit', 'foreign_request', 'source', 'session_length_in_minutes', 'device_os', 'keep_alive_session', 'device_distinct_emails_8w', 'device_fraud_count', 'month', 'x1', 'x2']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add account_id column to each file at the beginning of each file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Loop through each file and add the account_id column\n",
    "# for file in files:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    # df = pd.read_csv(os.path.join(directory, file))\n",
    "\n",
    "    # Add the account_id column where the value is 'ID' + the row number + 1 with leading zeros\n",
    "    # Create the account_id column with leading zeros\n",
    "    # account_id = ['ID' + str(i + 1).zfill(9) for i in range(len(df))]\n",
    "    # df.insert(0, 'account_id', account_id)\n",
    "\n",
    "    # Save the modified DataFrame back to the CSV file\n",
    "    # df.to_csv(os.path.join(directory, file), index=False)\n",
    "    # print(f\"Added account_id column to {file} and saved the changes.\")\n",
    "\n",
    "# Print the list of files with account_id added\n",
    "#print(\"List of files with account_id added:\")\n",
    "#for file in files:\n",
    "#    print(file)\n",
    "\n",
    "# Print  the column names of each file to verify the changes\n",
    "print(\"\\nFirst few rows of each file:\")\n",
    "for file in files:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(os.path.join(directory, file), nrows=5)\n",
    "\n",
    "    # Print the first few rows of the DataFrame\n",
    "    print(f\"First few rows of {file}:\")\n",
    "    print(df.head(5))\n",
    "    print(f\"File: {file}\")\n",
    "    print(\"Column names:\", df.columns.tolist())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64e57bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of Variant III.csv: (1000000, 34)\n",
      "Dimensions of Variant V.csv: (1000000, 34)\n"
     ]
    }
   ],
   "source": [
    "# Get the dimensions of the CSV files in the directory\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Iterate through the files and get the dimensions of each CSV file\n",
    "for file in files:\n",
    "    if file.endswith('.csv'):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(os.path.join(directory, file))\n",
    "        # Get the dimensions of the DataFrame\n",
    "        dimensions = df.shape\n",
    "        # Print the dimensions of the CSV file\n",
    "        print(f\"Dimensions of {file}: {dimensions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ca31a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common column names in all CSV files:\n",
      "source\n",
      "days_since_request\n",
      "x1\n",
      "velocity_24h\n",
      "intended_balcon_amount\n",
      "fraud_bool\n",
      "current_address_months_count\n",
      "proposed_credit_limit\n",
      "date_of_birth_distinct_emails_4w\n",
      "device_os\n",
      "name_email_similarity\n",
      "housing_status\n",
      "prev_address_months_count\n",
      "email_is_free\n",
      "velocity_6h\n",
      "session_length_in_minutes\n",
      "payment_type\n",
      "customer_age\n",
      "month\n",
      "device_fraud_count\n",
      "device_distinct_emails_8w\n",
      "employment_status\n",
      "income\n",
      "has_other_cards\n",
      "bank_branch_count_8w\n",
      "zip_count_4w\n",
      "phone_home_valid\n",
      "velocity_4w\n",
      "foreign_request\n",
      "credit_risk_score\n",
      "x2\n",
      "keep_alive_session\n",
      "bank_months_count\n",
      "phone_mobile_valid\n",
      "Column names that don't appear in all CSV files:\n"
     ]
    }
   ],
   "source": [
    "# Get the column names that appear in all of the CSV files\n",
    "\n",
    "# Initialize a set to store the column names\n",
    "common_columns = None\n",
    "# Iterate through the files and get the column names of each CSV file\n",
    "for file in files:\n",
    "    if file.endswith('.csv'):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(os.path.join(directory, file))\n",
    "        # Get the column names of the DataFrame\n",
    "        columns = set(df.columns)\n",
    "        # If this is the first file, initialize the common_columns set\n",
    "        if common_columns is None:\n",
    "            common_columns = columns\n",
    "        else:\n",
    "            # Update the common_columns set with the intersection of the current columns and the previous common_columns\n",
    "            common_columns.intersection_update(columns)\n",
    "\n",
    "# Print the common column names\n",
    "print(\"Common column names in all CSV files:\")\n",
    "for column in common_columns:\n",
    "    print(column)\n",
    "\n",
    "# Get the column names that don't appear in all of the CSV files\n",
    "# Initialize a set to store the column names that don't appear in all files\n",
    "non_common_columns = set()\n",
    "# Iterate through the files and get the column names of each CSV file\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith('.csv'):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(os.path.join(directory, file))\n",
    "        # Get the column names of the DataFrame\n",
    "        columns = set(df.columns)\n",
    "        # Update the non_common_columns set with the difference between the current columns and the common_columns\n",
    "        non_common_columns.update(columns - common_columns)\n",
    "\n",
    "# Print the non-common column names\n",
    "print(\"Column names that don't appear in all CSV files:\")\n",
    "for column in non_common_columns:\n",
    "    print(column)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c2f8c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34\n"
     ]
    }
   ],
   "source": [
    "print(len(non_common_columns), len(common_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4fed392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of the common columns, compare the distribution across the files and determine if they are statistically similar or dissimilar\n",
    "\n",
    "# import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# Initialize a dictionary to store the distributions of each common column across the files\n",
    "distributions = {column: {'summary_stats': [], 'one_hot': [], 'num_records': []} for column in common_columns}\n",
    "# Iterate through the files and get the distributions of each common column\n",
    "for file in files:\n",
    "    if file.endswith('.csv'):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(os.path.join(directory, file))\n",
    "        # Iterate through the common columns and get the distributions\n",
    "        for column in common_columns:\n",
    "            if column in df.columns:\n",
    "                # Get the summary statistics necessary to compare distributions of the column and append it to the list\n",
    "                # make sure to convert the column to numeric if it is not already\n",
    "                # if it is not numeric, and the number of unique values is less than 20, convert it to a one-hot encoding without dropping the first column\n",
    "                # otherwise, don't do anything with the column\n",
    "                # in summary stats, include the mean, std, min, 25%, 50%, 75%, max\n",
    "                if df[column].dtype == 'object':\n",
    "                    # Convert to numeric if possible\n",
    "                    try:\n",
    "                        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                if df[column].dtype == 'float64' or df[column].dtype == 'int64':\n",
    "                    # Get the summary statistics\n",
    "                    summary_stats = df[column].describe()\n",
    "                    # Append the summary statistics to the list\n",
    "                    distributions[column]['summary_stats'].append(summary_stats)\n",
    "                    # Get the number of records in the column\n",
    "                    num_records = df[column].count()\n",
    "                    # Append the number of records to the list\n",
    "                    distributions[column]['num_records'].append(num_records)\n",
    "                    distributions[column]['one_hot'].append(False)\n",
    "                elif df[column].dtype == 'object' and len(df[column].unique()) < 20:\n",
    "                    # Convert to one-hot encoding without dropping the first column\n",
    "                    one_hot = pd.get_dummies(df[column], prefix=column, drop_first=False)\n",
    "                    # Append the one-hot encoding to the list\n",
    "                    # distributions[column]['one_hot'].append(True)\n",
    "                    # Get the summary statistics of the one-hot encoding\n",
    "                    summary_stats = one_hot.describe()\n",
    "                    # Append the summary statistics to the list\n",
    "                    for one_hot_column in one_hot.columns:\n",
    "                        # Get the summary statistics of the one-hot column\n",
    "                        one_hot_summary_stats = one_hot[one_hot_column].describe()\n",
    "                        # Append the summary statistics to the list\n",
    "                        distributions[one_hot_column]['summary_stats'].append(one_hot_summary_stats)\n",
    "                        distributions[one_hot_column]['one_hot'].append(True)\n",
    "                        distributions[one_hot_column]['num_records'].append(num_records)\n",
    "\n",
    "                    # distributions[column]['summary_stats'].append(summary_stats)\n",
    "                    # Get the number of records in the column\n",
    "                    # num_records = df[column].count()\n",
    "                    # Append the number of records to the list\n",
    "                    # distributions[column]['num_records'].append(num_records)\n",
    "                else:\n",
    "                    # Don't do anything with the column\n",
    "                    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70a1b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the results of the statistical tests\n",
    "results = {column: {'ks_test': [], 'ttest': [], 'mannwhitneyu': []} for column in common_columns}\n",
    "# Iterate through the distributions and perform the statistical tests\n",
    "\n",
    "# Save the results to a file\n",
    "with open('statistical_test_results.txt', 'w') as f:\n",
    "    for column in distributions.keys():\n",
    "        # Get the summary statistics of the column\n",
    "        summary_stats = distributions[column]['summary_stats']\n",
    "        # Get the number of records in the column\n",
    "        num_records = distributions[column]['num_records']\n",
    "        # Get the one-hot encoding of the column\n",
    "        one_hot = distributions[column]['one_hot']\n",
    "        \n",
    "        # we need to use the num_records to get the right calculation for the ks test, ttest, and mannwhitneyu tests\n",
    "        # if the column is a one-hot encoding, we need to get the number of records for each one-hot column\n",
    "        # also if it isn't a one-hot encoding, we need to get the number of records for each column to do the ks test, ttest, and mannwhitneyu tests\n",
    "        # finally, we need to make a pairwise comparison of the equally named columns across the files through the use of the ks test, ttest, and mannwhitneyu tests\n",
    "\n",
    "        for i in range(len(summary_stats)):\n",
    "            # Get the summary statistics of the column\n",
    "            summary_stats_i = summary_stats[i]  # this is the summary statistics of the column for the ith file\n",
    "            # Get the number of records in the column\n",
    "            num_records_i = num_records[i]  # this is the number of records in the column for the ith file\n",
    "            # Get the one-hot encoding of the column\n",
    "            one_hot_i = one_hot[i]  # this is a boolean indicating if the column is a one-hot encoding or not for the ith file\n",
    "\n",
    "            # Perform the ks test, ttest, and mannwhitneyu tests\n",
    "            for j in range(i + 1, len(summary_stats)):\n",
    "                # Get the summary statistics of the column\n",
    "                summary_stats_j = summary_stats[j]  # this is the summary statistics of the column for the jth file\n",
    "                # Get the number of records in the column\n",
    "                num_records_j = num_records[j]  # this is the number of records in the column for the jth file\n",
    "                # Get the one-hot encoding of the column\n",
    "                one_hot_j = one_hot[j]   # this is a boolean indicating if the column is a one-hot encoding or not for the jth file\n",
    "\n",
    "                # since we are doing a pairwise comparison, and the columns have the same name, they also have the same type\n",
    "                # so we can just use the summary statistics to get the values for the ks test, ttest, and mannwhitneyu tests, regardless of the type of column it is\n",
    "                # Get the values for the ks test, ttest, and mannwhitneyu tests\n",
    "                \n",
    "                ks_stat, ks_p_value = stats.ks_2samp(distributions[column]['summary_stats'][i], distributions[column]['summary_stats'][j])\n",
    "                t_stat, t_p_value = stats.ttest_ind(distributions[column]['summary_stats'][i], distributions[column]['summary_stats'][j])\n",
    "                mw_stat, mw_p_value = stats.mannwhitneyu(distributions[column]['summary_stats'][i], distributions[column]['summary_stats'][j])\n",
    "\n",
    "                results[column]['ks_test'].append((ks_stat, ks_p_value))\n",
    "                results[column]['ttest'].append((t_stat, t_p_value))\n",
    "                results[column]['mannwhitneyu'].append((mw_stat, mw_p_value))\n",
    "\n",
    "\n",
    "                #f.write(f\"Results for {column}: KS Test: {ks_stat}, T-Test: {t_stat}, Mann-Whitney U: {mw_stat}\")\n",
    "                f.write(\"\\n\")\n",
    "                f.write(\"\\n\")\n",
    "                # f.write(f\"KS Test p-value: {ks_p_value}, T-Test p-value: {t_p_value}, Mann-Whitney U p-value: {mw_p_value}\")\n",
    "                f.write(\"\\n\")\n",
    "                f.write(\"\\n\")\n",
    "                # Check if the p-value is less than 0.05, if so, the distributions are statistically different\n",
    "                if ks_p_value < 0.05:\n",
    "                    f.write(f\"{column} in {files[i]} and {files[j]} different (KS Test {ks_p_value})\")\n",
    "                else:\n",
    "                    f.write(f\"{column} in {files[i]} and {files[j]} similar (KS Test {ks_p_value})\")\n",
    "                f.write(\"\\n\")\n",
    "                if t_p_value < 0.05:\n",
    "                    f.write(f\"{column} in {files[i]} and {files[j]} different (T-Test {t_p_value})\")\n",
    "                else:\n",
    "                    f.write(f\"{column} in {files[i]} and {files[j]} similar (T-Test {t_p_value})\")\n",
    "                f.write(\"\\n\")\n",
    "                if mw_p_value < 0.05:\n",
    "                    f.write(f\"{column} in {files[i]} and {files[j]} different (Mann-Whitney U Test {mw_p_value})\")\n",
    "                else:\n",
    "                    f.write(f\"{column} in {files[i]} and {files[j]} similar (Mann-Whitney U Test {mw_p_value})\")\n",
    "                f.write(\"\\n\")\n",
    "                f.write(\"\\n\")\n",
    "                f.write(\"\\n\")\n",
    "                #f.write(f\"Results for {column}:\\n\")\n",
    "                #f.write(f\"KS Test: {results[column]['ks_test']}\\n\")\n",
    "                #f.write(f\"T-Test: {results[column]['ttest']}\\n\")\n",
    "                #f.write(f\"Mann-Whitney U: {results[column]['mannwhitneyu']}\\n\")\n",
    "                #f.write(\"\\n\")\n",
    "                #f.write(\"\\n\")\n",
    "                #f.write(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14cc1ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for source:\n",
      "  KS Test: [(np.float64(nan), np.float64(nan))]\n",
      "  T-Test: [(np.float64(nan), np.float64(nan))]\n",
      "  Mann-Whitney U Test: [(np.float64(nan), np.float64(nan))]\n",
      "Results for days_since_request:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(-9.459625477233043e-07), np.float64(0.9999992585782878))]\n",
      "  Mann-Whitney U Test: [(np.float64(28.5), np.float64(0.7525377274994569))]\n",
      "Results for x1:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(8.33078000575493e-09), np.float64(0.9999999934705437))]\n",
      "  Mann-Whitney U Test: [(np.float64(34.5), np.float64(0.8332720830457769))]\n",
      "Results for velocity_24h:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(-1.8088172691440124e-05), np.float64(0.9999858229440488))]\n",
      "  Mann-Whitney U Test: [(np.float64(30.5), np.float64(0.9162977979978499))]\n",
      "Results for intended_balcon_amount:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(-3.4029191053235055e-07), np.float64(0.999999733287738))]\n",
      "  Mann-Whitney U Test: [(np.float64(28.5), np.float64(0.7525377274994569))]\n",
      "Results for fraud_bool:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(0.0), np.float64(1.0))]\n",
      "  Mann-Whitney U Test: [(np.float64(31.5), np.float64(1.0))]\n",
      "Results for current_address_months_count:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(2.1563346255045853e-06), np.float64(0.9999983099190195))]\n",
      "  Mann-Whitney U Test: [(np.float64(33.5), np.float64(0.9160510722818964))]\n",
      "Results for proposed_credit_limit:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(5.527998851794073e-07), np.float64(0.9999995667293189))]\n",
      "  Mann-Whitney U Test: [(np.float64(33.0), np.float64(0.9576844644369863))]\n",
      "Results for date_of_birth_distinct_emails_4w:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(1.4172045021173797e-06), np.float64(0.9999988892306666))]\n",
      "  Mann-Whitney U Test: [(np.float64(33.5), np.float64(0.9160510722818964))]\n",
      "Results for device_os:\n",
      "  KS Test: [(np.float64(nan), np.float64(nan))]\n",
      "  T-Test: [(np.float64(nan), np.float64(nan))]\n",
      "  Mann-Whitney U Test: [(np.float64(nan), np.float64(nan))]\n",
      "Results for name_email_similarity:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(6.429348306274573e-11), np.float64(0.9999999999496083))]\n",
      "  Mann-Whitney U Test: [(np.float64(31.5), np.float64(1.0))]\n",
      "Results for housing_status:\n",
      "  KS Test: [(np.float64(nan), np.float64(nan))]\n",
      "  T-Test: [(np.float64(nan), np.float64(nan))]\n",
      "  Mann-Whitney U Test: [(np.float64(nan), np.float64(nan))]\n",
      "Results for prev_address_months_count:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(1.0477812921335788e-05), np.float64(0.9999917877531042))]\n",
      "  Mann-Whitney U Test: [(np.float64(31.5), np.float64(1.0))]\n",
      "Results for email_is_free:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(1.1511496316035854e-10), np.float64(0.9999999999097757))]\n",
      "  Mann-Whitney U Test: [(np.float64(32.0), np.float64(1.0))]\n",
      "Results for velocity_6h:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(-6.044895684321994e-05), np.float64(0.9999526216242257))]\n",
      "  Mann-Whitney U Test: [(np.float64(30.5), np.float64(0.9162977979978499))]\n",
      "Results for session_length_in_minutes:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(1.6452360677038048e-06), np.float64(0.9999987105052466))]\n",
      "  Mann-Whitney U Test: [(np.float64(31.0), np.float64(0.9580602618255808))]\n",
      "Results for payment_type:\n",
      "  KS Test: [(np.float64(nan), np.float64(nan))]\n",
      "  T-Test: [(np.float64(nan), np.float64(nan))]\n",
      "  Mann-Whitney U Test: [(np.float64(nan), np.float64(nan))]\n",
      "Results for customer_age:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(7.173049681594187e-09), np.float64(0.9999999943779436))]\n",
      "  Mann-Whitney U Test: [(np.float64(32.0), np.float64(1.0))]\n",
      "Results for month:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(9.74155272125387e-13), np.float64(0.9999999999992365))]\n",
      "  Mann-Whitney U Test: [(np.float64(32.0), np.float64(1.0))]\n",
      "Results for device_fraud_count:\n",
      "  KS Test: [(np.float64(0.0), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(0.0), np.float64(1.0))]\n",
      "  Mann-Whitney U Test: [(np.float64(32.0), np.float64(1.0))]\n",
      "Results for device_distinct_emails_8w:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(-1.005930119454226e-09), np.float64(0.9999999992115772))]\n",
      "  Mann-Whitney U Test: [(np.float64(31.0), np.float64(0.9569015325339879))]\n",
      "Results for employment_status:\n",
      "  KS Test: [(np.float64(nan), np.float64(nan))]\n",
      "  T-Test: [(np.float64(nan), np.float64(nan))]\n",
      "  Mann-Whitney U Test: [(np.float64(nan), np.float64(nan))]\n",
      "Results for income:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(-1.0354654543282046e-10), np.float64(0.9999999999188428))]\n",
      "  Mann-Whitney U Test: [(np.float64(32.0), np.float64(1.0))]\n",
      "Results for has_other_cards:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(3.56441004883348e-10), np.float64(0.9999999997206305))]\n",
      "  Mann-Whitney U Test: [(np.float64(33.0), np.float64(0.9551957673931278))]\n",
      "Results for bank_branch_count_8w:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(-1.9161243440362563e-05), np.float64(0.9999849818981177))]\n",
      "  Mann-Whitney U Test: [(np.float64(31.5), np.float64(1.0))]\n",
      "Results for zip_count_4w:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(-0.0001287398159691726), np.float64(0.99989909696242))]\n",
      "  Mann-Whitney U Test: [(np.float64(31.5), np.float64(1.0))]\n",
      "Results for phone_home_valid:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(-4.5348291478675095e-10), np.float64(0.9999999996445714))]\n",
      "  Mann-Whitney U Test: [(np.float64(31.0), np.float64(0.9566307365921487))]\n",
      "Results for velocity_4w:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(8.19910681405948e-06), np.float64(0.9999935737457818))]\n",
      "  Mann-Whitney U Test: [(np.float64(33.0), np.float64(0.9580602618255808))]\n",
      "Results for foreign_request:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(-1.2794410647386484e-10), np.float64(0.9999999998997207))]\n",
      "  Mann-Whitney U Test: [(np.float64(31.0), np.float64(0.9551957673931278))]\n",
      "Results for credit_risk_score:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(-2.256998333481844e-09), np.float64(0.9999999982310213))]\n",
      "  Mann-Whitney U Test: [(np.float64(32.0), np.float64(1.0))]\n",
      "Results for x2:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(8.474788709998313e-09), np.float64(0.9999999933576733))]\n",
      "  Mann-Whitney U Test: [(np.float64(34.5), np.float64(0.8332720830457769))]\n",
      "Results for keep_alive_session:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(-2.383548896778633e-10), np.float64(0.9999999998131834))]\n",
      "  Mann-Whitney U Test: [(np.float64(32.0), np.float64(1.0))]\n",
      "Results for bank_months_count:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(1.4756240491675988e-09), np.float64(0.9999999988434429))]\n",
      "  Mann-Whitney U Test: [(np.float64(33.0), np.float64(0.9579361106814205))]\n",
      "Results for phone_mobile_valid:\n",
      "  KS Test: [(np.float64(0.125), np.float64(1.0))]\n",
      "  T-Test: [(np.float64(8.316929414122214e-12), np.float64(0.9999999999934814))]\n",
      "  Mann-Whitney U Test: [(np.float64(32.0), np.float64(1.0))]\n"
     ]
    }
   ],
   "source": [
    "# Print the results of the statistical tests\n",
    "for column in results.keys():\n",
    "    print(f\"Results for {column}:\")\n",
    "    print(f\"  KS Test: {results[column]['ks_test']}\")\n",
    "    print(f\"  T-Test: {results[column]['ttest']}\")\n",
    "    print(f\"  Mann-Whitney U Test: {results[column]['mannwhitneyu']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "199ed020",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Stop running.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mStop running.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: Stop running."
     ]
    }
   ],
   "source": [
    "assert False, \"Stop running.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Kolmogorov-Smirnov test on the summary statistics\n",
    "ks_test = stats.ks_2samp(summary_stats[0], summary_stats[1])\n",
    "results[column]['ks_test'].append(ks_test)\n",
    "\n",
    "# Perform the t-test on the summary statistics\n",
    "ttest = stats.ttest_ind(summary_stats[0], summary_stats[1])\n",
    "results[column]['ttest'].append(ttest)\n",
    "\n",
    "# Perform the Mann-Whitney U test on the summary statistics\n",
    "mannwhitneyu = stats.mannwhitneyu(summary_stats[0], summary_stats[1])\n",
    "results[column]['mannwhitneyu'].append(mannwhitneyu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93060f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[column] = pd.to_numeric(df[column], errors='coerce') # # distributions[column].append(df[column].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0fd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import sys\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import optuna\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.decomposition import PCA, FastICA, TruncatedSVD\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, LassoCV, ElasticNetCV\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier, GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, RFE, RFECV, VarianceThreshold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction import FeatureUnion, DictVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import check_array, check_X_y, check_is_fitted\n",
    "from sklearn.utils import safe_indexing, indexable, _safe_indexing\n",
    "from sklearn.utils import _safe_indexing, _num_samples, _array_api_dispatcher\n",
    "from sklearn.utils import _num_samples, _array_api_dispatcher\n",
    "from sklearn.utils import _array_api_dispatcher, _num_samples, _safe_indexing, _check_sample_weight, _is_array_api_array, _array_api\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
