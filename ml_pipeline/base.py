import pandas as pd
from .eda import eda
from .feature_engineering import feature_engineering
from .partitioning import partitioning
from .numeric_conversion import numeric_conversion
from .scaling import scaling
from .model_baseline import model_baseline
from .shap_explainability import shap_explainability
from .shap_selection import shap_selection
from .feature_correlation import feature_correlation
from .feature_select_cluster import feature_select_cluster
from .feature_select_threshold import feature_select_threshold
from .hyperparameter_tuning import hyperparameter_tuning
from .final_model import final_model


class MLPipeline:
    """
    Master orchestration object for configuration-driven ML pipeline execution.
    Stores full state, supports hash-based tracking, checkpointing, and recovery.
    """

    def __init__(self, config: dict):
        self.config = config
        self.dataframes: dict[str, pd.DataFrame] = {}
        self.paths: dict[str, str] = {}
        self.hashes: dict[str, str] = {}
        self.models: dict[str, object] = {}
        self.metrics: dict[str, dict] = {}
        # self.metadata: dict[str, dict] = {}
        self.artifacts: dict[str, dict] = {}  # Store paths to all artifacts generated by each step
        self.transformations: dict[str, dict] = {}  # Store transformation parameters for each step
        self.artifact_dir = "artifacts"
        self.artifact_dirs: list[str] = [
            "mlruns",
            "artifacts",
            "artifacts/eda",
            "artifacts/step1",
            "artifacts/step2",
            "artifacts/step3",
            "artifacts/step4",
            "artifacts/step5",
            "artifacts/step6",
            "artifacts/step7",
            "artifacts/step8",
            "artifacts/step9",
            "artifacts/step10",
            "artifacts/step11",
            "artifacts/step12"
        ]
        

    def load_data(self, db_path: str = "fraud_poc.db") -> pd.DataFrame:
        """
        Load and merge raw data from SQLite into a single dataframe.
        """
        import sqlite3
        conn = sqlite3.connect(db_path)
        df_clients = pd.read_sql("SELECT * FROM clients", conn)
        df_merchants = pd.read_sql("SELECT * FROM merchants", conn)
        df_tx = pd.read_sql("SELECT * FROM transactions", conn)
        conn.close()

        df_clients.rename(columns={"account_creation_date": "account_creation_date_client"}, inplace=True)
        df_merchants.rename(columns={"account_creation_date": "account_creation_date_merchant"}, inplace=True)

        return df_tx.merge(df_clients, on="client_id").merge(df_merchants, on="merchant_id")

    def run_all(self) -> None:
        """
        Execute all pipeline steps in sequence, storing full internal state.
        """
        eda(self)
        feature_engineering(self)
        partitioning(self)
        numeric_conversion(self)
        scaling(self)
        model_baseline(self)
        
    
    def run_later(self)-> None:
        shap_explainability(self)
        shap_selection(self)
        feature_correlation(self)
        if self.config["use_cluster_select"][0]:
            feature_select_cluster(self)
        else:
            feature_select_threshold(self)
        hyperparameter_tuning(self)
        final_model(self)
        
        # Save the final state of the pipeline
        # self.save_state()