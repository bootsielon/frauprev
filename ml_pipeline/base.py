import pandas as pd
from .eda import eda
from .feature_engineering import feature_engineering
from .partitioning import partitioning
from .numeric_conversion import numeric_conversion
from .scaling import scaling
from .model_baseline import model_baseline
from .shap_explainability import shap_explainability
from .shap_selection import shap_selection
from .feature_correlation import feature_correlation
from .feature_select_cluster import feature_select_cluster
from .feature_select_threshold import feature_select_threshold
from .hyperparameter_tuning import hyperparameter_tuning
from .final_model import final_model
import datetime
from .utils import make_param_hash  # , log_registry

class MLPipeline:
    """
    Master orchestration object for configuration-driven ML pipeline execution.
    Stores full state, supports hash-based tracking, checkpointing, and recovery.
    """
    def __init__(
            self, 
            config: dict, 
            data_source: str = "sqlite", 
            db_path: str = "fraud_poc.db",
            xlsx_path: str = None,
            csv_paths: dict = None,
            raw_data: pd.DataFrame = None
    ):  # def __init__(self, config: dict):
        self.config = config
        self.train_mode = self.config.get('train_mode') if self.config.get('train_mode') is not None else True
        self.config["train_mode"] = self.train_mode
        self.data_source = data_source  # "sqlite", "xlsx", "csv", etc.
        self.raw_data = raw_data        # Optional: pass DataFrame directly
        self.db_path = db_path
        self.xlsx_path = xlsx_path
        self.csv_paths = csv_paths
        self.raw_data = raw_data        
        # ...rest of your init...        self.train = config['train_mode']
        self.dataframes: dict[str, pd.DataFrame] = {}
        self.paths: dict[str, str] = {}
        self.hashes: dict[str, str] = {}
        self.models: dict[str, object] = {}
        self.metrics: dict[str, dict] = {}
        # self.metadata: dict[str, dict] = {}
        self.artifacts: dict[str, dict] = {}  # Store paths to all artifacts generated by each step

        # Generate a global run hash based on timestamp and basic config
        global_hash_base = {
            "timestamp": datetime.now().isoformat(),
            "train_mode": self.train_mode,
            "data_source": self.data_source,
            "db_path": self.db_path if self.db_path and not self.xlsx_path and not self.csv_paths else None,
            "xlsx_path": self.xlsx_path if self.xlsx_path else None,
            "csv_paths": self.csv_paths if self.csv_paths else None,
            "config": self.config,  # Include the config dict itself for reproducibility
            
            # Add other critical config elements that should be consistent across a run
        }
        
        # Generate a hash ID for the current configuration
        self.global_hash = make_param_hash(global_hash_base)
        # If in inference mode, use the provided train_hash as the global_hash to maintain consistency
        self.global_train_hash = self.config.get("train_hash") if not self.train_mode else self.global_hash
        # Store the global hash in the config for components to access
        self.config["global_hash"] = self.global_hash

        self.transformations: dict[str, dict] = {}  # Store transformation parameters for each step
        self.artifact_dir = "artifacts"
        self.artifact_dirs: list[str] = [
            "mlruns",
            "artifacts",
            "artifacts/eda",
            "artifacts/step1",
            "artifacts/step2",
            "artifacts/step3",
            "artifacts/step4",
            "artifacts/step5",
            "artifacts/step6",
            "artifacts/step7",
            "artifacts/step8",
            "artifacts/step9",
            "artifacts/step10",
            "artifacts/step11",
            "artifacts/step12"
            ]
        # Bind the imported functions to this instance
        self.eda = lambda: eda(self)
        self.feature_engineering = lambda: feature_engineering(self)
        self.partitioning = lambda: partitioning(self)
        self.numeric_conversion = lambda: numeric_conversion(self)
        self.scaling = lambda: scaling(self)
        self.model_baseline = lambda: model_baseline(self)
        self.shap_explainability = lambda: shap_explainability(self)
        self.shap_selection = lambda: shap_selection(self)
        self.feature_correlation = lambda: feature_correlation(self)
        self.feature_select_cluster = lambda: feature_select_cluster(self)
        self.feature_select_threshold = lambda: feature_select_threshold(self)
        self.hyperparameter_tuning = lambda: hyperparameter_tuning(self)
        self.final_model = lambda: final_model(self)


    def load_data(self):  # , db_path: str = "fraud_poc.db", xlsx_path: str = None, csv_paths: dict = None) -> pd.DataFrame:
        """
        Load and merge raw data from SQLite, Excel, or CSV into a single dataframe.
        """
        db_path = self.db_path
        xlsx_path = self.xlsx_path
        csv_paths = self.csv_paths
        if self.data_source == "sqlite":
            import sqlite3
            conn = sqlite3.connect(db_path)
            df_clients = pd.read_sql("SELECT * FROM clients", conn)
            df_merchants = pd.read_sql("SELECT * FROM merchants", conn)
            df_tx = pd.read_sql("SELECT * FROM transactions", conn)
            conn.close()
            df_clients.rename(columns={"account_creation_date": "account_creation_date_client"}, inplace=True)
            df_merchants.rename(columns={"account_creation_date": "account_creation_date_merchant"}, inplace=True)
            return df_tx.merge(df_clients, on="client_id").merge(df_merchants, on="merchant_id")
        elif self.data_source == "xlsx":
            xls = pd.ExcelFile(xlsx_path)
            df_clients = pd.read_excel(xls, sheet_name="clients")
            df_merchants = pd.read_excel(xls, sheet_name="merchants")
            df_tx = pd.read_excel(xls, sheet_name="transactions")
            df_clients.rename(columns={"account_creation_date": "account_creation_date_client"}, inplace=True)
            df_merchants.rename(columns={"account_creation_date": "account_creation_date_merchant"}, inplace=True)
            return df_tx.merge(df_clients, on="client_id").merge(df_merchants, on="merchant_id")
        elif self.data_source == "csv":
            df_clients = pd.read_csv(csv_paths["clients"])
            df_merchants = pd.read_csv(csv_paths["merchants"])
            df_tx = pd.read_csv(csv_paths["transactions"])
            df_clients.rename(columns={"account_creation_date": "account_creation_date_client"}, inplace=True)
            df_merchants.rename(columns={"account_creation_date": "account_creation_date_merchant"}, inplace=True)
            return df_tx.merge(df_clients, on="client_id").merge(df_merchants, on="merchant_id")
        elif self.data_source == "raw":
            return self.raw_data
        else:
            raise ValueError(f"Unknown data_source: {self.data_source}")
    

    def load_data_(self, db_path: str = "fraud_poc.db") -> pd.DataFrame:
        """
        Load and merge raw data from SQLite into a single dataframe.
        """
        import sqlite3
        conn = sqlite3.connect(db_path)
        df_clients = pd.read_sql("SELECT * FROM clients", conn)
        df_merchants = pd.read_sql("SELECT * FROM merchants", conn)
        df_tx = pd.read_sql("SELECT * FROM transactions", conn)
        conn.close()

        df_clients.rename(columns={"account_creation_date": "account_creation_date_client"}, inplace=True)
        df_merchants.rename(columns={"account_creation_date": "account_creation_date_merchant"}, inplace=True)

        return df_tx.merge(df_clients, on="client_id").merge(df_merchants, on="merchant_id")


    def run_all(self) -> None:
        """
        Execute all pipeline steps in sequence, storing full internal state.
        """
        self.eda()
        self.feature_engineering()
        self.partitioning()
        self.numeric_conversion()
        self.scaling()
        self.model_baseline()

    
    def run_later(self)-> None:
        self.shap_explainability()
        self.shap_selection()
        self.feature_correlation()
        if self.config["use_cluster_select"][0]:
            self.feature_select_cluster()
        else:
            self.feature_select_threshold()
        self.hyperparameter_tuning()
        self.final_model()
        
        # Save the final state of the pipeline
        # self.save_state()